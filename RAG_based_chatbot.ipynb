{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "#loader = PyPDFLoader(file_path=\"drive/MyDrive/Colab Notebooks/ds-book.pdf\")\n",
    "loader = PyPDFLoader(file_path=\"travel-policy.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50, separator=\"\\n\")\n",
    "docs = text_splitter.split_documents(documents=documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "#embeddings = OpenAIEmbeddings()\n",
    "\n",
    "embeddings= HuggingFaceEmbeddings()\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "vectorstore.save_local(\"faiss_store1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.combine_documents.stuff import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"faiss_store1\", embeddings=embeddings, allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # mmr or similarity_score_threshold\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "message = \"\"\"\n",
    "        Answer this question using the provided context only.\n",
    "        If the information is not available in the context, just reply with \"i dont know\"\n",
    "        {input}\n",
    "        Context:\n",
    "        {context}\n",
    "        \"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\", message)],\n",
    ")\n",
    "llm = ChatOpenAI()\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "#print(question_answer_chain)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "print(rag_chain)\n",
    "\n",
    "# rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "# rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"tell me about all the reimbursement policies\"})\n",
    "print(response)\n",
    "print(response['answer'])\n",
    "for  doc in response[\"context\"]:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def load_pdf_into_vectorstore(file: tempfile) -> str:\n",
    "    try:\n",
    "        print(\"======Loading file==================\")\n",
    "        file_path = file.name\n",
    "        loader = PyPDFLoader(file_path=file_path)\n",
    "        documents = loader.load()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n",
    "        docs = text_splitter.split_documents(documents=documents)\n",
    "        embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "        #vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents, embedding=embeddings , persist_directory=\"chromadb11\"\n",
    "        )\n",
    "       # vectorstore.save_local(\"pdf_store\")\n",
    "\n",
    "        print(\"======File Loaded================== \")\n",
    "\n",
    "        return 'Document uploaded and index created successfully. You can chat now.'\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "\n",
    "def getresponse(query, history:list) -> tuple:\n",
    "\n",
    "   vectorstore = Chroma(\n",
    "      persist_directory=\"chromadb11\", embedding_function=embeddings\n",
    "  )\n",
    "\n",
    " #  vectorstore = FAISS.load_local(\"pdf_store\", embeddings= OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
    "\n",
    "   message = \"\"\"\n",
    "    Answer this question using the provided context . If information is not available in the context,\n",
    "      Just respond saying \"I dont know\"\n",
    "    {input}\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "\n",
    "   prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "   llm = ChatOpenAI()\n",
    "\n",
    "\n",
    "   question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "#print(question_answer_chain)\n",
    "   rag_chain = create_retrieval_chain(vectorstore.as_retriever(), question_answer_chain)\n",
    "\n",
    "   # rag_chain = (\n",
    "   #    {\"context\": vectorstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "   #    | prompt\n",
    "   #    | llm\n",
    "   # )\n",
    "\n",
    "\n",
    "\n",
    "   response1= rag_chain.invoke({\"input\":query})\n",
    "   print(response1)\n",
    "   history.append((query, response1['answer']))\n",
    "   return \"\",history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            file = gr.components.File(\n",
    "                label='Upload your pdf file',\n",
    "                file_count='single',\n",
    "                file_types=['.pdf'])\n",
    "            #with gr.Row():\n",
    "            upload = gr.components.Button(\n",
    "                    value='Upload', variant='primary')\n",
    "\n",
    "        label = gr.components.Textbox()\n",
    "    chatbot = gr.Chatbot(label='Talk to the Document')\n",
    "\n",
    "\n",
    "    msg = gr.Textbox()\n",
    "    clear = gr.ClearButton([msg, chatbot])\n",
    "    vectorStore =None\n",
    "\n",
    "    upload.click(load_pdf_into_vectorstore,[file],[label])\n",
    "\n",
    "    msg.submit(getresponse, [msg,chatbot], [msg, chatbot])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
